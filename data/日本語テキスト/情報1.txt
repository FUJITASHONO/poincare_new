The 20th Annual Conference of the Japanese Society for Artificial Intelligence, 2006 - 1 - 模倣学習と強化学習の調和による効率的行動獲得Efficient Acquisition of Behaviors by Harmonizing Reinforcement Learning with Imitation Learning 田渕 一真*1 谷口 忠大*1 椹木 哲夫*1 Kazuma Tabuchi Tadahiro Taniguchi Tetsuo Sawaragi *1 京都大学大学院工学研究科Graduate School of Engineering, Kyoto University This paper presents a composite machine learning architecture of imitation learning and reinforcement learning. Humans usually learn tasks through both imitation learning and reinforcement learning. After observing superiors, learners start practicing through trial and error. In this context, imitation learning and reinforcement learning seem harmonized as a smooth series of learning phases. From the viewpoint of machine learning usually requires many trials and errors in an agent’s learning phase. However, imitating other people’s way of performing the task can reduce the amount of time. Based on this idea, the composition of reinforcement learning and imitation learning is proposed as an integrated machine learning architecture. An additional reward system is introduced, which connects these learning algorithms more naturally. 1. はじめに人間が技能を獲得するプロセスには様々な形態が考えられる．その中でも，”学ぶ（まなぶ）”という言葉と”まねぶ”が同源である事が示唆するように，模倣を通じて他者から学習することは， 人間の技能の獲得において本質的役割を演じている．この模倣学習のプロセスを計算論的に理解する事は，自律適応的なロボットの設計論として重要なだけでなく，人間組織の技能伝承のプロセスを理解する上でも重要である． 自律ロボットの学習方法としての模倣学習研究を見ると，模倣とはしばしば教師あり学習のプロセスのみで定式化される．しかし，我々が他者の行動を真似て学習するプロセスとは，そのような受け身的な学習に終始しない．もし，模倣学習が師匠から弟子，弟子からその弟子への教師あり学習の連鎖で表現されるとすれば，その技能は次第に劣化していくのみである．この劣化プロセスは印刷機でコピーのコピーをとるといった作業に等しい．稲邑らはモーションキャプチャによって人間の動作から教師事例を生成し，HMM を通してその運動を記憶させるミメシスモデルを提案している[稲邑 2004]．この例を含め，多くの模倣学習の研究では模倣される他者を完全な教師と見なし，完全なコピーを取ることを模倣学習としている．しかし，実際の我々の学習プロセスを振り返ると，むしろ，模倣は他者の行動を参考にするための方法にすぎず，参考にした後に自ら試行錯誤を行うことでより洗練された行動へと変化させていくプロセスこそ模倣学習において本質的である．つまり，模倣学習は試行錯誤に基づく強化学習と融合して初めて一連の学習プロセスを形成すると考えられる． 本研究では，他のエージェントの行動を観察し模倣学習を行い，その後に試行錯誤に基づく強化学習により修正していくというプロセスを一つの学習器の上で実現する手法を提案する． さらに，学習者が模倣対象エージェントの行動観察結果を強化学習時に積極的に利用するための補助的な報酬を導入することによって，これら二つの異質な学習則を調和させる．これによって他者の行動を真似つつ，さらに自らの試行錯誤を通じて洗練させていく効率的な学習手法を検討する． 2. 模倣学習と強化学習の融合本研究で検討するモデルでは，まず観察した他者の行動を真似ることで大まかに望ましい行動に近づき，次いで副報酬を伴った強化学習を行うことで，模倣によって得た方策を洗練していくという順で学習を行う．なお，学習器には線形近似器を用い， 最急降下法によって学習する． 2.1 模倣学習模倣学習はその名の通り他者の行動を真似た行動をとること ができるようになるための学習であるが，その特徴は，模倣を行う場合には模倣する対象は理想的な行動を行っていると一時的に見なすことにある．模倣学習は模倣される他者がいて初めて成り立つ学習であるから，他のエージェントが居ない場合や，エージェントの集団にとって全く新規の行動を学ぶ時には用いることが出来ない．その点では適用できる状況が限られる学習法である． 模倣学習では他者の行動を観察し，それと同様の行動が出来るように行動形成を行う．しかし，通常，運動学習において学習主体が模倣対象となる他者の運動出力としての出力信号を直接計測することは不可能であり，外部の観察者としてその行動出力を観察できるのみである場合が多い．例えば，人間は模倣を行うときに，他の人間が脳から筋肉に出力した筋電位を計測したり，手が実際に出したトルクを計測したりすることはない． 3C1-2 連絡先： 〒606-8253 京都市左京区吉田本町 京都大学大学院工学研究科 機械理工学専攻 機械システム創成学分野 田渕一真 Tel, Fax: 075-753-5044 E-mail: kazuma@102514.mbox.media.kyoto-u.ac.jp 図1. 模倣学習から強化学習へ模倣学習強化学習他者の行動の参照The 20th Annual Conference of the Japanese Society for Artificial Intelligence, 2006 - 2 - また，仮に計測できたとしても，自身の身体と他者の身体の相違から同じ運動指令を出力する事が同じ行動結果を生み出すとは限らない． これに対して谷口らは身体の持つダイナミクスに依存しない行為概念表現としてセンサ空間上でのアトラクタを行動の表現単位として獲得することの有効性を動的環境下における強化学習の文脈から提案し，これを汎化行為概念と呼んでいる[谷口2006]．また，中西らは同様に行為を表象する状態空間上のアトラクタを運動プリミティブと呼び，その獲得についての研究を行っている[中西 2004]．本研究では，この方式をとる事で学習者と被模倣者の身体の違いを吸収し模倣学習をモデル化する． エージェントは他者の行動の結果としての軌跡を観察することで，エージェント内部にセンサ空間上のアトラクタの一部を獲得し，保持する．行動時にはこれを駆動し，目標となるセンサ入力値を逐次，逆モデルに入力し，対応する運動出力を出力させることで行動する．逆モデルはモデル学習を行うことにより獲得することが可能であり，事前の設計を要さない． 本稿では，タスクと無関係に学習が可能な逆モデル（inverse model）をあらかじめ学習しておいた後，ある状態に対して目標とするセンサ入力値と模倣する対象のデータを逐次比較することによって軌道決定モジュール(state-actor)の学習を行い，模倣行動を獲得する．すなわち，現在の状態をt s ，観測した模倣対象の行動後の状態をsup t+1 s ，軌道決定モジュールの持つ線型近似器の重みをi θ ，基底関数を i ( t ) f s とすると，目標とするセンサ入力値1 . t+ s および1 回の学習における重みの更新値i Δθ は， . ( ) 1 i t i t i s =Σθ f s + (1) ( . ) ( ) 1 supi t 1 t i t Δθ s s f s + + ∝ − (2) と表される． このような特徴を持つ模倣学習は，直接的で高速な学習を可能とするが，この方法はいくつかの欠点を持つ．一つは，模倣する対象が望ましい状態に素早く到達するような理想的な行動をとっているとは限らないこと．もう一つは仮に理想的な行動をとっていたとしても，観察できる回数は少なく，エージェント自身が取り得る状態全てについての観察事例を獲得する事は叶わず， 全状態空間中で観察事例が粗にしか存在しないということ．ゆえに，観察模倣学習自体は完全であったとしても，それによって模倣したエージェントが常に理想的な行動を行い，タスクを達成出来るようになる保証はない．以上より，模倣学習のみで可能な学習には限界がある． よって，これを乗り越えるためにはエージェントが自ら試行錯誤を通じて追加の学習を行う必要がある． 2.2 強化学習上記のような模倣学習の欠点を補う方法のひとつとして，他者の存在に依存しない自律的で試行錯誤的な強化学習を用いることが考えられる．強化学習は期待累積報酬の最大化を目指した目的指向的学習で，かつ経験した状態行動対のみをオンラインで評価する経験ベースの学習であるため，タスクに習熟した他者の存在を前提とせず，自らの行う試行錯誤のみを通じて意味のある行動を獲得することができるという特徴を持つ．しかし，学習の結果として獲得される挙動に関する規範が陽に与えられる訳ではないので，環境全体にわたってしらみつぶしに分析する必要がある．そのため，複雑で多次元な問題における学習では学習速度が急激に減少してしまう欠点がある[Sutton 1998]．本研究では，模倣学習の後に強化学習を行うことによって，模倣学習の持つ欠点を補う事を目指すが，これは強化学習の欠点を補うものでもある． 本稿では強化学習の手法としては，方策(actor)と価値関数(critic)を独立に持つActor-Critic 学習法[木村 2000]を用いる． Actor-Critic 学習法を採用することで一つの学習器で模倣による教師あり学習と強化学習両方を実現させる事が出来る．ただし，本モデルにおけるactor の出力は(1)式で表される目標とするセンサ入力値1 . t + s とする．本稿ではこのように次状態を出力としてだすactor をstate-actor とよぶ事にする．環境への出力は逆モデルを通して行われる．すなわち，現在の状態t s について， 報酬をt r ，割引率をγ，critic の持つ線型近似器の重みをi w ，基底関数を ( ) i t φ s とすると， t s における価値 ( t ) V s ，TD 誤差t δおよび重みの更新値i Δw は， ( ) ( ) i t i t i V s =Σwφ s (3) ( ) ( ) t t 1 t 1 t = r + V s −V s + + δ γ (4) ( ) i t i t Δw ∝δ φ s (5) と表され，強化学習時において，現在の状態がt s のときの運動出力後の次状態をt+1 s とおくと，state-actor の重みの変化量i Δθ は， ( . ) ( ) i t t 1 t 1 i t Δ s s f s + + θ ∝δ − (6) と表される． 2.3 副報酬の導入上のように教師あり学習をベースに，強化学習を行わせた例としては村田らの人間の歩行データを元にした二足歩行の強化学習による獲得などがある[村田 2005]．しかし，模倣によって獲得できる行動は状態空間において局所的なものであり，状態空間全域を網羅したものではない．そのため，模倣学習を行い， その後で強化学習を別に行う，といったような単純な組み合わせでは，その学習内容を強化学習時には十分に利用できない可能性がある．そこで，強化学習時において他者の行動の観察データをより積極的に利用するための手段として観察経験に基づいた副報酬の概念の導入する． 副報酬導入の問題点として，副報酬を加えて学習した場合と加えずに学習した場合において報酬が最大となる状態が異なる可能性があることが挙げられる．副報酬を導入したことでタスクの課題自体が変化し，異なる解に収束することとなる．ゆえに， 副報酬は報酬の最大化に関して整合性を持たせた形で導入しなければならない．この問題に対してNg, Harada, Russell は， 状 態 の み に 依 存 す る 関 数 ( ) t Φ s を 用 い る 場 合 ， ( ) ( ) t 1 t Φ s − Φ s + γ を副報酬として主報酬と足しあわせる，つまり新たな報酬関数t r を図2. 模倣と強化による統合学習器の概観critic t r t s 1 . t+ s t state- a actor reinforcement learning reinforcement learning environment imitation learning inverse model model learning imitator ( sup ) t a sup t s observed agent supt+1 s t δ≒The 20th Annual Conference of the Japanese Society for Artificial Intelligence, 2006 - 3 - ( ) ( ) t 1 t 1 t 1 t r = r + Φ s −Φ s + + + γ (7) として主報酬に代えて与えれば，最適行動に影響を与えることなく収束速度を変化させることが可能であることを示した[Ng 1999]．ただし，(7)式に従うことによって最適行動が変化しないことは保証されるが，学習が効率化するとは限らない．すなわち， ( ) t Φ s の設計によって学習速度は高速化することも低速化する こともあり得るものである． 本研究では，模倣する対象を観察して得られたデータの量を用いて副報酬を作成する．具体的には，状態空間を分割し，その領域に属する観察データの数 ( t ) D s と，最も観察データの属する数が多い領域に属するデータ数maxD(s)との比から得ら れるΦ =ζ +ξ max ( ) ( ) ( )s s sD D t t (8) (ξ, ζは任意の定数)を用いて副報酬を生成する．この副報酬は， 観察された模倣によって学習した行動の系列へ引き込まれるような行動が強化学習によって生まれることを意図したものである． この引き込みによって，主報酬のもとでの最適行動を変化させることなく，強化学習における探索対象となる領域について模倣対象が与えられていない領域から与えられている領域への遷移を促す．なお，この副報酬は外部から与えられるのではなく， エージェント内部で生成されるものである． 2.4 学習機構と手順以上から，本研究で検討する学習機構は，ある状態に対して目標とする次状態のセンサ入力値を出力するstate-actor，現在の状態と目標とする次状態から行動出力を決定するinverse model，Actor-Critic 学習法におけるcritic の3 つのモジュールで構成される．そして学習は， 1．タスクに関しての学習を始める前に，逆モデルについて学習を行う． 2．観測した模倣対象に関するデータとstate-actor の出力を比較することで模倣学習を行う． 3．観測した模倣対象に関するデータの状態空間における分布から ( ) t Φ s を求め，副報酬を伴った強化学習を行う． という手順で行う． 3. 実験上記のモデルを，シミュレーション上における倒立振子の振り 上げタスクに適用し，その有効性を検証した． 3.1 実験内容と評価方法(1) タスク環境と報酬設計本実験でタスクとする倒立振子は，移動方向が水平線上に拘束された台車に一様断面の物理振子を取り付け，水平方向の力を加えることで振子を振り上げる方式のものである． このモデルにおける運動方程式は，振子の長さをl ，台車および振子の質量，速度減衰係数をそれぞれ s p M,m,c ,c ，重力 加速度をg とすると以下のように表される． 0. 2cos sin cos 1 23 cos 2cos sin ( ) 1 2( ) 1 2 + + + + = + + − + + + = θ θ θ θ θ θ θ θ θ θ l && && & l & && l && & & l& p p s p p m mx mg c x c M m x m c c x c x u エージェントが試行を開始するときには台車の位置x ，速度x& ， 振子の角度θ ，角速度θ& の初期値を無作為に与えた．また一定時間試行を行うと，再び無作為な初期値を与えてそこから学習させるようにした． 報酬関数は原点において振り上げた状態で最大となるよう[exp{ ( )2} exp{ ( )2} cos ] exp( 2 ) t t t t t r =α −β θ −π + −β θ +π − θ +ε −φx (α ,β ,ε ,φ は設計者の決定する任意の定数)と与えた． (2) エージェントの設計エージェントの測定できる変数はx, x&,θ,θ& とした．基底関数に は図3 のような三角形状のものを用いた．また，基底の配置は倒立振子の振り上げが可能だと考えられる状態空間の範囲と基底数を設計者が選び，各変数に関して等間隔に配置した． (3) 模倣される他者模倣対象のデータを得るために，本実験を行う前にActor- Critic 学習法を用いてある程度学習させたエージェントを準備する．このエージェントは決して最適な行動を取ることの出来るエージェントではない．これに振り上げを実行させ，その振り上げの状態遷移の過程から模倣を行うエージェントの行動決定時間周期と同じ間隔で，模倣するエージェントにx, x&,θ,θ&をサンプ リングさせ，模倣学習の為の観測事例として獲得させた．また， 基底配置を行った状態空間の範囲と同様の範囲を各変数に関して等分割し，分割された各範囲について観察データから( t ) D s を求めた． (4) 評価方法あらかじめ逆モデルの学習，模倣学習を行わせたのち，強化学習を行い，一定時間ごとの獲得報酬をサンプリングした．また比較対象として，Φ( ) = 0 t s としたこと以外は上記のモデルと同じ条件とした模倣学習は行うが副報酬は利用しないエージェントと， Actor-Critic 学習法のみで学習するエージェントについて同様の実験を行った． なお，強化学習と模倣学習は目的の異なる学習方法であり比較できないため，強化学習時のデータのみをサンプリングし，比較した． 3.2 結果と考察α= 0.5, β= 50, ε= 0.3, φ= 2.5×10-5, γ= 0.9, ζ= 0.5, ξ= -0.05， 線形近似器の学習率を0.1，力学モデルの更新幅を0.01，その表1. 基底配置，状態分割のパラメータ設計変数 最小値最大値 基底数 状態分割x -2.0×106 2.0×106 31 15 x& -5.0×105 5.0×105 31 9 θ -2π 2π 15 7 θ& -1.0×102 1.0×102 31 7 図4. 基底関数図 3. タスクモデルl = 4.0×103 m = 40 M = 1.0×103 = 3.2 s c = 45 p c物理パラメータ s st fi (st) wiwi-1 fi-1(st) 10The 20th Annual Conference of the Japanese Society for Artificial Intelligence, 2006 - 4 - 他のパラメータを図2，表1 のように設計し，各モデルについて4 回ずつ行った実験結果の平均が図4 である． この結果より，模倣学習を行ったエージェントの学習速度は， 行っていないエージェントに比べ高速な立ち上がりを見せた．また副報酬を用いていないエージェントのグラフは，本研究のモデルを用いたエージェントのものと比べ波形の上下が激しかった．これは，副報酬を導入していない場合，エージェントは模倣が可能な状態に移行する・模倣状態を維持するための機構を持っていないため，模倣対象を真似る行動をとった場合には高い報酬が得られるものの，模倣状態に素早く到達・維持できるとは限らないため学習が不安定化するという現象が起きている可能性が示唆された．さらに，本研究のモデルを用いたエージェントのグラフの波形は，Actor-Critic 学習法のみで学習したものと比べて最終的な学習結果が安定しなかった．これは，模倣対象のエージェントの行動がActor-Critic 学習法により獲得されたものであるために，このエージェントの行動が最適方策に基づいたものであるとは言い難く，Ng, Harada, Russell らの定理が最適方策の不変性は示しているものの，この未熟な模倣対象への依存が主報酬の最大化を阻害したのではないかと考えられる． しかし，学習者の模倣対象への依存性や，副報酬の設計の学習プロセスへの影響についてはさらなる研究が必要である． 4. おわりに本研究では，機械学習の文脈で，これまでは独立に考えられがちであった強化学習と模倣学習が抱える問題について，両者を融合させることによって相補的に改善する学習手法を検討した．未だ検討の余地は多分に残されているものの，模倣学習を行動学習に役立てる際に最適方策の学習効率の向上が可能であること，副報酬を導入することで模倣学習と強化学習とを調和させながら接続することが可能であることを簡単な例題によって検証した．しかし，今回提案した手法でも，模倣される対象が必ずタスクに関して習熟していることを前提し，それを盲目的に学習者が真似るという従来からの模倣学習が持つ構造は変化していない．これに対し，人間の学習時の行動を考えると，模倣する価値のある対象を選び，選択的に模倣するといった一段メタな学習機構が存在すると考えられる．このようなメタな模倣学習の機構が，人間の模倣学習を通した組織内での技能伝播といった社会的な現象にも強く結びついていると考えられる．人間の模倣学習の構成論的研究という視点からすれば，今後はそのようなメタな選択も含めた模倣学習のモデル化を目指していきたいと筆者らは考えている． 参考文献 [稲邑 2004] 稲邑哲也，他:“ミメシス理論に基づく見まね学習とシンボル創発の統合モデル”，日本ロボット学会誌，vol.22, no.2, pp.256—263,2004. [谷口 2006] 谷口忠大，椹木哲夫: “汎化行為概念の適応的獲得.双シェマモデルベースの強化学習.”，計測制御学会論文集，vol.44, no.3, pp.255—264, 2006. [中西 2004] 中西淳, 他，“運動学習プリミティブを用いたロボットの見まね学習”, 日本ロボット学会誌, vol.22，no.2. pp. 165—170, 2004. [Sutton 1998] R. Sutton, A.G. Barto, “Reinforcement Learning : An Introduction”, The MIT Press, 1998. [木村 2000] 木村元，小林重信, “Actor に適正度の履歴を用いたActor-Critic アルゴリズム：不完全なValue-Function のもとでの強化学習” , 人工知能学会誌, vol. 15, no. 2, pp.267--275, 2000, [Ng 1999] A.Y.Ng, D.Harada and S.Russell: "Policy invariance under reward transformations:Theory and application to reward shaping", In Proceedings of the Sixteenth International Conference on Machine Learning, 1999. [村田 2005] 村田栄理，他，“強化学習による多自由度2 足歩行ロボットの制御“，第19 回日本人工知能学会全国大会予稿集， in CD-ROM，2004. -2 -101234567 0 50 100 150 200 250 エピソード数累積獲得報酬 [104] -2 -101234567 0 50 100 150 200 250 エピソード数累積獲得報酬[104] -2 -101234567 0 50 100 150 200 250 エピソード数累積獲得報酬[104] 図5. Actor-Critic 学習法のみで学習したモデル(上)， Φ( ) = 0 t s としたモデル(中)，提案するモデル(下)の獲得報酬の推移