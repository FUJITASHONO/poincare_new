The 20th Annual Conference of the Japanese Society for Artificial Intelligence, 2006 - 1 - 模倣学習と強化学習の調和による効率的行動獲得Efficient Acquisition of Behaviors by Harmonizing Reinforcement Learning with Imitation Learning 田渕 一真*1 谷口 忠大*1 椹木 哲夫*1 Kazuma Tabuchi Tadahiro Taniguchi Tetsuo Sawaragi *1 京都大学大学院工学研究科Graduate School of Engineering, Kyoto University This paper presents a composite machine learning architecture of imitation learning and reinforcement learning. Humans usually learn tasks through both imitation learning and reinforcement learning. After observing superiors, learners start practicing through trial and error. In this context, imitation learning and reinforcement learning seem harmonized as a smooth series of learning phases. From the viewpoint of machine learning usually requires many trials and errors in an agent’s learning phase. However, imitating other people’s way of performing the task can reduce the amount of time. Based on this idea, the composition of reinforcement learning and imitation learning is proposed as an integrated machine learning architecture. An additional reward system is introduced, which connects these learning algorithms more naturally. 1. はじめに人間が技能を獲得するプロセスには様々な形態が考えられる．その中でも，”学ぶ（まなぶ）”という言葉と”まねぶ”が同源である事が示唆するように，模倣を通じて他者から学習することは， 人間の技能の獲得において本質的役割を演じている．この模倣学習のプロセスを計算論的に理解する事は，自律適応的なロボットの設計論として重要なだけでなく，人間組織の技能伝承のプロセスを理解する上でも重要である． 自律ロボットの学習方法としての模倣学習研究を見ると，模倣とはしばしば教師あり学習のプロセスのみで定式化される．しかし，我々が他者の行動を真似て学習するプロセスとは，そのような受け身的な学習に終始しない．もし，模倣学習が師匠から弟子，弟子からその弟子への教師あり学習の連鎖で表現されるとすれば，その技能は次第に劣化していくのみである．この劣化プロセスは印刷機でコピーのコピーをとるといった作業に等しい．稲邑らはモーションキャプチャによって人間の動作から教師事例を生成し，HMM を通してその運動を記憶させるミメシスモデルを提案している[稲邑 2004]．この例を含め，多くの模倣学習の研究では模倣される他者を完全な教師と見なし，完全なコピーを取ることを模倣学習としている．しかし，実際の我々の学習プロセスを振り返ると，むしろ，模倣は他者の行動を参考にするための方法にすぎず，参考にした後に自ら試行錯誤を行うことでより洗練された行動へと変化させていくプロセスこそ模倣学習において本質的である．つまり，模倣学習は試行錯誤に基づく強化学習と融合して初めて一連の学習プロセスを形成すると考えられる． 本研究では，他のエージェントの行動を観察し模倣学習を行い，その後に試行錯誤に基づく強化学習により修正していくというプロセスを一つの学習器の上で実現する手法を提案する． さらに，学習者が模倣対象エージェントの行動観察結果を強化学習時に積極的に利用するための補助的な報酬を導入することによって，これら二つの異質な学習則を調和させる．これによって他者の行動を真似つつ，さらに自らの試行錯誤を通じて洗練させていく効率的な学習手法を検討する． 2. 模倣学習と強化学習の融合本研究で検討するモデルでは，まず観察した他者の行動を真似ることで大まかに望ましい行動に近づき，次いで副報酬を伴った強化学習を行うことで，模倣によって得た方策を洗練していくという順で学習を行う．なお，学習器には線形近似器を用い， 最急降下法によって学習する． 2.1 模倣学習模倣学習はその名の通り他者の行動を真似た行動をとること ができるようになるための学習であるが，その特徴は，模倣を行う場合には模倣する対象は理想的な行動を行っていると一時的に見なすことにある．模倣学習は模倣される他者がいて初めて成り立つ学習であるから，他のエージェントが居ない場合や，エージェントの集団にとって全く新規の行動を学ぶ時には用いることが出来ない．その点では適用できる状況が限られる学習法である． 模倣学習では他者の行動を観察し，それと同様の行動が出来るように行動形成を行う．しかし，通常，運動学習において学習主体が模倣対象となる他者の運動出力としての出力信号を直接計測することは不可能であり，外部の観察者としてその行動出力を観察できるのみである場合が多い．例えば，人間は模倣を行うときに，他の人間が脳から筋肉に出力した筋電位を計測したり，手が実際に出したトルクを計測したりすることはない． 3C1-2 連絡先： 〒606-8253 京都市左京区吉田本町 京都大学大学院工学研究科 機械理工学専攻 機械システム創成学分野 田渕一真 Tel, Fax: 075-753-5044 E-mail: kazuma@102514.mbox.media.kyoto-u.ac.jp 図1. 模倣学習から強化学習へ模倣学習強化学習他者の行動の参照The 20th Annual Conference of the Japanese Society for Artificial Intelligence, 2006 - 2 - また，仮に計測できたとしても，自身の身体と他者の身体の相違から同じ運動指令を出力する事が同じ行動結果を生み出すとは限らない． これに対して谷口らは身体の持つダイナミクスに依存しない行為概念表現としてセンサ空間上でのアトラクタを行動の表現単位として獲得することの有効性を動的環境下における強化学習の文脈から提案し，これを汎化行為概念と呼んでいる[谷口2006]．また，中西らは同様に行為を表象する状態空間上のアトラクタを運動プリミティブと呼び，その獲得についての研究を行っている[中西 2004]．本研究では，この方式をとる事で学習者と被模倣者の身体の違いを吸収し模倣学習をモデル化する． エージェントは他者の行動の結果としての軌跡を観察することで，エージェント内部にセンサ空間上のアトラクタの一部を獲得し，保持する．行動時にはこれを駆動し，目標となるセンサ入力値を逐次，逆モデルに入力し，対応する運動出力を出力させることで行動する．逆モデルはモデル学習を行うことにより獲得することが可能であり，事前の設計を要さない． 本稿では，タスクと無関係に学習が可能な逆モデル（inverse model）をあらかじめ学習しておいた後，ある状態に対して目標とするセンサ入力値と模倣する対象のデータを逐次比較することによって軌道決定モジュール(state-actor)の学習を行い，模倣行動を獲得する．すなわち，現在の状態をt s ，観測した模倣対象の行動後の状態をsup t+1 s ，軌道決定モジュールの持つ線型近似器の重みをi θ ，基底関数を i ( t ) f s とすると，目標とするセンサ入力値1 . t+ s および1 回の学習における重みの更新値i Δθ は， . ( ) 1 i t i t i s =Σθ f s + (1) ( . ) ( ) 1 supi t 1 t i t Δθ s s f s + + ∝ − (2) と表される． このような特徴を持つ模倣学習は，直接的で高速な学習を可能とするが，この方法はいくつかの欠点を持つ．一つは，模倣する対象が望ましい状態に素早く到達するような理想的な行動をとっているとは限らないこと．もう一つは仮に理想的な行動をとっていたとしても，観察できる回数は少なく，エージェント自身が取り得る状態全てについての観察事例を獲得する事は叶わず， 全状態空間中で観察事例が粗にしか存在しないということ．ゆえに，観察模倣学習自体は完全であったとしても，それによって模倣したエージェントが常に理想的な行動を行い，タスクを達成出来るようになる保証はない．以上より，模倣学習のみで可能な学習には限界がある． よって，これを乗り越えるためにはエージェントが自ら試行錯誤を通じて追加の学習を行う必要がある． 2.2 強化学習上記のような模倣学習の欠点を補う方法のひとつとして，他者の存在に依存しない自律的で試行錯誤的な強化学習を用いることが考えられる．強化学習は期待累積報酬の最大化を目指した目的指向的学習で，かつ経験した状態行動対のみをオンラインで評価する経験ベースの学習であるため，タスクに習熟した他者の存在を前提とせず，自らの行う試行錯誤のみを通じて意味のある行動を獲得することができるという特徴を持つ．しかし，学習の結果として獲得される挙動に関する規範が陽に与えられる訳ではないので，環境全体にわたってしらみつぶしに分析する必要がある．そのため，複雑で多次元な問題における学習では学習速度が急激に減少してしまう欠点がある[Sutton 1998]．本研究では，模倣学習の後に強化学習を行うことによって，模倣学習の持つ欠点を補う事を目指すが，これは強化学習の欠点を補うものでもある． 本稿では強化学習の手法としては，方策(actor)と価値関数(critic)を独立に持つActor-Critic 学習法[木村 2000]を用いる． Actor-Critic 学習法を採用することで一つの学習器で模倣による教師あり学習と強化学習両方を実現させる事が出来る．ただし，本モデルにおけるactor の出力は(1)式で表される目標とするセンサ入力値1 . t + s とする．本稿ではこのように次状態を出力としてだすactor をstate-actor とよぶ事にする．環境への出力は逆モデルを通して行われる．すなわち，現在の状態t s について， 報酬をt r ，割引率をγ，critic の持つ線型近似器の重みをi w ，基底関数を ( ) i t φ s とすると， t s における価値 ( t ) V s ，TD 誤差t δおよび重みの更新値i Δw は， ( ) ( ) i t i t i V s =Σwφ s (3) ( ) ( ) t t 1 t 1 t = r + V s −V s + + δ γ (4) ( ) i t i t Δw ∝δ φ s (5) と表され，強化学習時において，現在の状態がt s のときの運動出力後の次状態をt+1 s とおくと，state-actor の重みの変化量i Δθ は， ( . ) ( ) i t t 1 t 1 i t Δ s s f s + + θ ∝δ − (6) と表される． 2.3 副報酬の導入上のように教師あり学習をベースに，強化学習を行わせた例としては村田らの人間の歩行データを元にした二足歩行の強化学習による獲得などがある[村田 2005]．しかし，模倣によって獲得できる行動は状態空間において局所的なものであり，状態空間全域を網羅したものではない．そのため，模倣学習を行い， その後で強化学習を別に行う，といったような単純な組み合わせでは，その学習内容を強化学習時には十分に利用できない可能性がある．そこで，強化学習時において他者の行動の観察データをより積極的に利用するための手段として観察経験に基づいた副報酬の概念の導入する． 副報酬導入の問題点として，副報酬を加えて学習した場合と加えずに学習した場合において報酬が最大となる状態が異なる可能性があることが挙げられる．副報酬を導入したことでタスクの課題自体が変化し，異なる解に収束することとなる．ゆえに， 副報酬は報酬の最大化に関して整合性を持たせた形で導入しなければならない．この問題に対してNg, Harada, Russell は， 状 態 の み に 依 存 す る 関 数 ( ) t Φ s を 用 い る 場 合 ， ( ) ( ) t 1 t Φ s − Φ s + γ を副報酬として主報酬と足しあわせる，つまり新たな報酬関数t r を